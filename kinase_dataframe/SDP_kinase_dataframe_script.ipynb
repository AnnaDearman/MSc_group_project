{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BIO727P - Bioinformatics Software Development Group Project (2019/20)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### AIM: To retrive information about human protein kinases from various databases and compile into one table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python version: Python 3.7.4\n",
    "\n",
    "# import the required packages\n",
    "\n",
    "!pip install pandas\n",
    "\n",
    "import pandas as pd # import pandas\n",
    "import re # import regular expression\n",
    "import urllib.request # import url library module"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Step 1:__ Compile a list of human protein kinases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Step 1a:__ Use UniProt to gather a list of entry names and accession numbers for the protein kinases listed on the URL specified in the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url=\"https://www.uniprot.org/docs/pkinfam.txt\" # retrieve webpage of human kinases from UniProt\n",
    "webpage=urllib.request.urlopen(url) # open the URL\n",
    "myfile=webpage.read().decode() # read the URL contents\n",
    "\n",
    "hum_kin=re.findall(r\"([A-Z0-9]+)_HUMAN\", myfile) # create a regular expression to find all human kinases and extract all to a list\n",
    "\n",
    "uni_id=re.findall(r\"_HUMAN\\s+\\((.*?)\\)\", myfile)\n",
    "\n",
    "human_kinases=[] # open an empty list to store UniProt identifiers for all human kinases\n",
    "uniprot_id=[] # open an empty list to store UniProt accession numbers for all human kinases\n",
    "\n",
    "for h in range(len(hum_kin)):\n",
    "    human_kinases.append(hum_kin[h]+\"_HUMAN\") # append identifiers to new list, re-adding the \"_HUMAN\" found on all identifiers\n",
    "    uniprot_id.append(uni_id[h].replace(\" \", \"\")) # append accession numbers to new list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Step 1b:__ Append a list of additional kinases found from phosphosite webscraping to existing lists using acquired UniProt entry names and accession numbers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extra_kinases=pd.read_csv(\"extra_kinases.csv\") # open the extra kinase list \n",
    "extra_entries_list=list(extra_kinases.Entry_name) # extract extra entry names as a list\n",
    "extra_accessions_list=list(extra_kinases.uniprot_id) # extract extra entry names as a list\n",
    "    \n",
    "for k in range(len(extra_accessions_list)):\n",
    "    uniprot_id.append(extra_accessions_list[k]) # append extra accession numbers to list\n",
    "    human_kinases.append(extra_entries_list[k]) # append extra identifiers to list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Step 1c:__ Create a dataframe with the UniProt entries and accession numbers - this is the basis for the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.DataFrame({'Entry_name' : human_kinases}) # create a new table with the header entry name consisting of the human_kinases list\n",
    "df['UniProt_ID']=uniprot_id # add the uniprot id list as a column to the dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Step 1d:__ Create a dataframe with the UniProt accession numbers only - this is so it is possible to convert UniProtKB AC/ID to PDB IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1=pd.DataFrame({'UniProt_ID' : uniprot_id})\n",
    "df1.to_csv(\"uniprot_acc_nums.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Step 2:__ Extract following kinase information from UniProt:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Step 2a:__ Protein names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "primary_protein_name=[] # open two empty lists for the two different types of names seen\n",
    "alternative_protein_names=[]\n",
    "\n",
    "for x in range(len(uniprot_id)):\n",
    "    protein_url=\"https://www.uniprot.org/uniprot/?query=id:\"+uniprot_id[x]+\"&columns=protein%20names&format=tab\" # retrieve webpage\n",
    "    protein_webpage=urllib.request.urlopen(protein_url) # open the URL\n",
    "    protein_file=protein_webpage.read().decode() # read the URL contents \n",
    "        \n",
    "    protein_list=protein_file.split(\"\\n\") # seperate protein name from header\n",
    "    \n",
    "    pattern1=re.compile(r\"\\[(Cleaved into|Includes): (.*?)\\]\") # regular expression for notes within protein names\n",
    "    prot_names1=pattern1.sub(\"\", (str(protein_list[1]))) # remove the notes from the string\n",
    "    \n",
    "    pattern2=re.compile(r\"\\(EC(.*?)\\)\") # regular expression for EC numbers\n",
    "    prot_names2=pattern2.sub(\"\", prot_names1) # remove EC numbers from the string\n",
    "    \n",
    "    # certain names have brackets within their names, and to avoid getting recognised by the regular expression, they need\n",
    "    # to be replaced by curly brackets\n",
    "    pattern3=re.compile(r\"\\((.*?)\\)]\") \n",
    "    prot_names3=pattern3.sub(\"{acetyl-transferring}]\", prot_names2)\n",
    "    \n",
    "    pattern4=re.compile(r\"\\(A\\)\")\n",
    "    prot_names4=pattern4.sub(\"{A}\", prot_names3)\n",
    "    \n",
    "    pattern5=re.compile(r\"\\(C\\)\")\n",
    "    prot_names5=pattern5.sub(\"{C}\", prot_names4)\n",
    "    \n",
    "    pattern6=re.compile(r\"\\(II\\)\")\n",
    "    prot_names6=pattern6.sub(\"{II}\", prot_names5)\n",
    "    \n",
    "    pattern7=re.compile(r\"\\(v-fgr\\)\")\n",
    "    prot_names7=pattern7.sub(\"{v-fgr}\", prot_names6)\n",
    "    \n",
    "    # exclude all the alternative names that are contained within regular brackets\n",
    "    pattern8=re.compile(r\"\\((.*?)\\)\")\n",
    "    prot_names8=pattern8.sub(\"\", prot_names7)\n",
    "    \n",
    "    prot_names9=prot_names8.replace(\"{\", \"(\").replace(\"}\", \")\").replace(\"' \", \"\") # replace punctuation in the string\n",
    "\n",
    "    primary_protein_name.append(prot_names9) # append primary protein names to list\n",
    "    \n",
    "    matches=re.findall(r\"\\((.*?)\\)\", prot_names7) # find all alternate protein names contained within brackets \n",
    "    alt_prot_names=(str(matches)).replace(\"[\", \"\").replace(\"]\", \"\").replace(\"'\", \"\").replace(\"{\", \"(\").replace(\"}\", \")\").replace('\"', \"\")\n",
    "        # replace punctuation in the string \n",
    "    alternative_protein_names.append(alt_prot_names) # append alternative protein names to list\n",
    "\n",
    "df['Primary_Protein_Name']=primary_protein_name # add the primary names list as a column to the dataframe\n",
    "df['Alternative_Protein_Name(s)']=alternative_protein_names # add the alternate names list as a column to the dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Step 2b:__ Gene symbols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# primary gene symbol\n",
    "\n",
    "primary_gene_name=[] # open empty list for primary gene names\n",
    "\n",
    "for p in range(len(uniprot_id)):\n",
    "    gene_url=\"https://www.uniprot.org/uniprot/?query=id:\"+uniprot_id[p]+\"&columns=genes(PREFERRED)&format=tab\" # retrieve webpage\n",
    "    gene_webpage=urllib.request.urlopen(gene_url) # open the URL\n",
    "    gene_file=gene_webpage.read().decode() # read the URL contents \n",
    "    \n",
    "    gene_list=gene_file.split(\"\\n\") # seperate gene name from header\n",
    "    \n",
    "    primary_gene_name.append(str(gene_list[1])) # append gene name to the new list\n",
    "\n",
    "# alternate gene symbol(s)\n",
    "\n",
    "alternative_gene_names=[] # open new list for alternative gene names\n",
    "\n",
    "for a in range(len(uniprot_id)):\n",
    "    gene_alt_url=\"https://www.uniprot.org/uniprot/?query=id:\"+uniprot_id[a]+\"&columns=genes(ALTERNATIVE)&format=tab\" # retrieve webpage\n",
    "    gene_alt_webpage=urllib.request.urlopen(gene_alt_url) # open the URL\n",
    "    gene_alt_file=gene_alt_webpage.read().decode() # read the URL contents \n",
    "    \n",
    "    gene_alt_list=gene_alt_file.split(\"\\n\") # seperate gene name from header\n",
    "    \n",
    "    gene_alt_names1=(str(gene_alt_list[1])).split(\" \") # split by space\n",
    "    gene_alt_names2=(str(gene_alt_names1)).replace(\"[\", \"\").replace(\"]\", \"\").replace(\"'\", \"\")\n",
    "    alternative_gene_names.append(gene_alt_names2) # append gene names to the new list\n",
    "\n",
    "df['Gene_Symbol']=primary_gene_name # add the primary names list as a column to the dataframe\n",
    "df['Alternative_Gene_Name(s)']=alternative_gene_names # add the alternate names list as a column to the dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Step 2c:__ Families"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "family=[] # open new list for family names \n",
    "\n",
    "for f in range(len(uniprot_id)):\n",
    "    fam_url=\"https://www.uniprot.org/uniprot/?query=id:\"+uniprot_id[f]+\"&columns=families&format=tab\" # retrieve webpage\n",
    "    fam_webpage=urllib.request.urlopen(fam_url) # open the URL\n",
    "    fam_file=fam_webpage.read().decode() # read the URL contents \n",
    "    \n",
    "    fam_list=fam_file.split(\"\\n\") # seperate family name from header\n",
    "    \n",
    "    family.append(fam_list[1]) # append family names to a new list\n",
    "    \n",
    "df['Families']=family # add the families names list as a column to the dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Step 2d:__ Sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence=[] # open new list for sequence\n",
    "\n",
    "for s in range(len(uniprot_id)):\n",
    "    seq_url=\"https://www.uniprot.org/uniprot/?query=id:\"+uniprot_id[s]+\"&columns=sequence&format=tab\" # retrieve webpage\n",
    "    seq_webpage=urllib.request.urlopen(seq_url) # open the URL\n",
    "    seq_file=seq_webpage.read().decode() # read the URL contents \n",
    "    \n",
    "    seq_list=seq_file.split(\"\\n\") # seperate sequence name from header\n",
    "    \n",
    "    sequence.append(seq_list[1]) # append sequences to a new list\n",
    "\n",
    "df['AA_Seq']=sequence # add the AA seq list as a column to the dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Step 2e:__ Molecular Mass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "molecular_mass=[] # open new list for molecular mass\n",
    "\n",
    "for m in range(len(uniprot_id)):\n",
    "    mass_url=\"https://www.uniprot.org/uniprot/?query=id:\"+uniprot_id[m]+\"&columns=mass&format=tab\" # retrieve webpage\n",
    "    mass_webpage=urllib.request.urlopen(mass_url) # open the URL\n",
    "    mass_file=mass_webpage.read().decode() # read the URL contents \n",
    "    \n",
    "    mass_list=mass_file.split(\"\\n\") # seperate sequence name from header\n",
    "    \n",
    "    molecular_mass.append(mass_list[1]) # append masses to a new list\n",
    "    \n",
    "df['Molecular_Mass_(Da)']=molecular_mass # add the mass list as a column to the dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Step 2f:__ Subcellular location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_cell_locs=pd.read_csv(\"subcellular_locations.csv\") # open the locations list \n",
    "sub_cell_locs_list=list(sub_cell_locs.Location) # extract locations as a list\n",
    "\n",
    "subcellular_location=[] # open new list for subcellular location \n",
    "\n",
    "for l in range(len(uniprot_id)):\n",
    "    cell_url=\"https://www.uniprot.org/uniprot/?query=id:\"+uniprot_id[l]+\"&columns=comment(SUBCELLULAR%20LOCATION)&format=tab\" # retrieve webpage\n",
    "    cell_webpage=urllib.request.urlopen(cell_url) # open the URL\n",
    "    cell_file=cell_webpage.read().decode() # read the URL contents \n",
    "    \n",
    "    cell_list=cell_file.split(\"\\n\") # seperate sequence name from header\n",
    "    cell_locations1=(str(cell_list[1])).replace(\"SUBCELLULAR LOCATION: \", \"\") # turn the locations to a string \n",
    "    \n",
    "    pattern5=re.compile(r\"Note=(.*?).+\") # regular expression to remove notes found in the subcellular locations part of UniProt\n",
    "    cellular_locations2=pattern5.sub(\"\", cell_locations1) # remove notes found in the subcellular locations part of UniProt \n",
    "    \n",
    "    cellular_locations3=str([loc for loc in sub_cell_locs_list if(loc in cellular_locations2)]) # if a part of the string contains\n",
    "        # an item found in the subcellular locations list, then add to a list and turn it into a string \n",
    "    subcellular_location.append(cellular_locations3.replace(\"[\", \"\").replace(\"]\", \"\").replace(\"'\", \"\")) # replace punctuation in string\n",
    "\n",
    "df['Subcellular_Location']=subcellular_location # add the subcellular location list as a column to the dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Step 3:__ Acquiring structure images from Protein Data Bank (PDB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Step 3a:__ By going onto UniProt.org, and using the 'retrieve/ID mapping' function, it is possible to turn UniProt IDs into PDB IDs. To do this, use the 'uniprot_acc_nums.csv' file created earlier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Step 3b:__ Download the two lists: UniProt IDs that were mapped, and IDs that were not able to be mapped. Then load in the mapped list using pandas. As one UniProt ID maps to multiple PDB IDs, we can remove the duplicates and set up lists required to acquire the different information needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdb=pd.read_csv(\"uniprot_to_pdb.csv\") # load in the list of uniprot IDs converted to PDB IDs\n",
    "\n",
    "pdb_df = pdb.drop_duplicates(subset=['From'], keep='first') # remove the duplicates\n",
    "\n",
    "final_pdb = pdb_df.rename(columns={'From': 'UniProt_ID', 'To': 'PDB_ID'}) # rename column headers\n",
    "\n",
    "pdb_id=list(final_pdb.PDB_ID) # turn the PDB column into a list\n",
    "uniprot=list(final_pdb.UniProt_ID) # turn the uniprot column into a list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Step 3c:__ Construct URLs to acquire the images, and extract the titles of the structures from the HTML source code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdb_url=[]\n",
    "pdb_title=[]\n",
    "\n",
    "for z in range(len(pdb_id)):\n",
    "    pdb_url.append(\"https://cdn.rcsb.org/pdb/images/\"+pdb_id[z]+\"_asym_r_500.jpg\") # construct the url to acquire images and \n",
    "                                                                                   # append to url list\n",
    "    \n",
    "    structure_url=\"https://www.rcsb.org/structure/\"+pdb_id[z] # retrieve webpage of kinase structures\n",
    "    structure_webpage=urllib.request.urlopen(structure_url) # open the URL\n",
    "    structure_file=structure_webpage.read().decode() # read the URL contents\n",
    "    \n",
    "    pattern=re.compile(r\"content=\\\"(.*?): (.*?)\\\"\") # regular expression to acquire the structure title\n",
    "    match=pattern.search(structure_file) # match the title\n",
    "    title=match.group(2) # assign the second match group to a new object\n",
    "    \n",
    "    title_string=(str(title)).replace(\"[\", \"\").replace(\"]\", \"\").replace(\"'\", \"\").replace('\"', \"\") # tidy up the title entry\n",
    "    pdb_title.append(title_string) # append to the pdb title list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Step 3d:__ Open the unmapped UniProt IDs that were unable to be mapped using pandas, and append them to the uniprot list. Also add empty strings to the ID and URL lists, and in the title list add a message saying no PDB structure available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "not_in_pdb=pd.read_csv(\"uniprot_not_mapped.csv\") # load in the list of uniprot IDs that couldn't be mapped to pdb ids\n",
    "not_in_pdb_list=list(not_in_pdb.not_mapped) # turn it into a python list\n",
    "\n",
    "for j in range(len(not_in_pdb_list)):\n",
    "    uniprot.append(not_in_pdb_list[j]) # append the uniprot IDs to list\n",
    "    pdb_id.append(\"\") # append nothing to the pdb id list\n",
    "    pdb_url.append(\"\") # append nothing to the pdb url list\n",
    "    pdb_title.append(\"No Protein Data Bank structure available.\") # append message to the pdb title list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Step 3e:__ Create a new table appending all the UniProt IDs, PDB IDs, URLs and titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "structure=pd.DataFrame({'UniProt_ID' : uniprot}) # make a new pandas table \n",
    "structure['PDB_ID']=pdb_id # add the following columns from the lists constructed above\n",
    "structure['PDB_URL']=pdb_url\n",
    "structure['PDB_title']=pdb_title"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Step 4:__ Merge the two dataframes to acquire the final dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df=structure.merge(df, on=\"UniProt_ID\") # merge the two dataframes based on the uniprot IDs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Step 5:__ Export the pandas dataframe to a .csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.to_csv(\"human_kinase_dataframe.csv\", index=False) # export pandas table to .csv and exclude indexing values as a column"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
